{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Learning\n",
        "\n",
        "Text Cleaning\n",
        "\n",
        "- removing unnecessary texts\n",
        "- Symbols\n",
        "- Emjois\n",
        "- lowercasing/uppercasing\n",
        "- remove noise\n",
        "- Breaking text in individual words or subwords ( tokenization)\n",
        "- Removing common or uninformative words\n",
        "- Stemming (reducing word to root form)\n",
        "- Fixing spell erros\n",
        "- Remving HTML tags or URLS\n",
        "- Handling contraction\n",
        "- handling emojis and Emotions\n",
        "- Handling out of vocab words\n",
        "- Handling multiple languages"
      ],
      "metadata": {
        "id": "mFoRdH9VD1Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing symbols and punctuations\n",
        "\n",
        "punct = \"Hey!, lets remove puctuations & symbols from text. okaay?\"\n",
        "\n",
        "remove_punct = \"!@#$%^&*(),.~``?><;;\"\n",
        "\n",
        "cleaned_sen = \"\"\n",
        "for hello in punct:\n",
        "  if(hello not in remove_punct):\n",
        "      cleaned_sen += hello\n",
        "print(cleaned_sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXBD-HRCGFuD",
        "outputId": "3c9101b1-7e81-4206-878f-e40afdc8459a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey lets remove puctuations  symbols from text okaay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing symbols and punctuations usingRegex\n",
        "import regex\n",
        "\n",
        "#regex.sub() replcase accurance of pattern\n",
        "punct_1 = regex.sub(r'[^\\w\\s]','',punct)\n",
        "\n",
        "print(punct_1)\n",
        "\n",
        "#regex.match()\n",
        "punct_2 = regex.match(\"remove\", punct)\n",
        "\n",
        "print(punct_2)\n",
        "\n",
        "\n",
        "#regex.search()\n",
        "\n",
        "punct_3 = regex.search(\"okaay\", punct)\n",
        "print(punct_3)\n",
        "\n",
        "#similarly there is rgex.findall(), regex.split() , regex. findite()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2d4xKrtIM8b",
        "outputId": "2ea94d50-336c-4b1a-dbe8-576e757404db"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey lets remove puctuations  symbols from text okaay\n",
            "None\n",
            "<regex.Match object; span=(51, 56), match='okaay'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Tokenization : Tokenization helps to reduce the complexity of the text data, enabling NLP models to work with discrete units rather than processing raw text as a continuous sequence of characters. It also helps improve the efficiency and accuracy of NLP models and algorithms in various downstream tasks.\n",
        "  Word Tokenization: Breaking the text into individual words. The most common method for word tokenization is space-based tokenization.\n",
        "\n",
        "- Sentence Tokenization: Splitting the text into individual sentences.\n",
        "\n",
        "- Subword Tokenization: Breaking words into smaller subword units, useful for handling morphologically rich languages or out-of-vocabulary (OOV) words.\n",
        "\n",
        "- Character Tokenization: Treating each character in the text as a separate token.\n",
        "\n",
        "- Custom Tokenization: Tailoring tokenization based on specific domain or task requirements.\n",
        "   "
      ],
      "metadata": {
        "id": "Fl-n5qw9IIAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC10acCVphCG",
        "outputId": "c1153909-a8ad-4168-cd52-0e4c490ee037"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYpXDFgHj-zw",
        "outputId": "edaec65f-ce5b-45fa-c555-ebbcd20adaab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer, RegexpTokenizer\n"
      ],
      "metadata": {
        "id": "rJnE29_qoq1R"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text\n",
        "Text = \"I work in PF. It's amazing place to be a part of.\"\n",
        "\n",
        "#sentence tokenization\n",
        "\n",
        "sen_token = sent_tokenize(Text)\n",
        "print(sen_token)\n",
        "\n",
        "#word splitiing\n",
        "word_token = word_tokenize(Text)\n",
        "print(word_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wezZ1g4oo8pi",
        "outputId": "861fc2fb-4524-4714-87ab-f1fca5aef66e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I work in PF.', \"It's amazing place to be a part of.\"]\n",
            "['I', 'work', 'in', 'PF', '.', 'It', \"'s\", 'amazing', 'place', 'to', 'be', 'a', 'part', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming\n",
        "Stemming is a text normalization technique used in Natural Language Processing (NLP) to reduce words to their base or root form, known as the stem. The stem may not always be a valid word, but it represents the core meaning of related words. For example, the stem of \"running,\" \"runs,\" and \"ran\" is \"run.\""
      ],
      "metadata": {
        "id": "MLRchM98vS0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"jumps\", \"jumping\", \"jumped\", \"happy\", \"happier\", \"happiest\"]\n",
        "\n",
        "# Perform stemming for each word\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Print the stemmed words\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "id": "MxryHBWEv_vS",
        "outputId": "436bbfe6-23d3-44e5-9ec7-6ca7f381c669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'ran', 'jump', 'jump', 'jump', 'happi', 'happier', 'happiest']\n"
          ]
        }
      ]
    }
  ]
}